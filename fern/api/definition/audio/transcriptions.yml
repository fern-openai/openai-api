# yaml-language-server: $schema=https://raw.githubusercontent.com/fern-api/fern/main/fern.schema.json

imports: 
  commons: ../commons.yml 

service:
  auth: true
  base-path: /audio
  endpoints:
    create:
      path: /transcriptions
      method: POST
      docs: Transcribes audio into the input language.
      request:
        body: CreateTranscriptionRequest
      response: CreateTranscriptionResponse

types: 
  CreateTranscriptionRequest: 
    properties:
      file: 
        type: commons.AudioFile 
        docs: The audio file to transcribe.
      model:
        type: string 
        docs: | 
          ID of the model to use. Only `whisper-1` is currently available.
      prompt: 
        type: optional<string>
        docs: | 
          An optional text to guide the model's style or continue a previous audio segment. 
          The [prompt](https://platform.openai.com/docs/guides/speech-to-text/prompting) should match the audio language.
      response_format: 
        type: optional<commons.FileFormat> 
        docs: |
          The format of the transcript output.
      temperature: 
        type: optional<double>
        docs: | 
          The sampling temperature, between 0 and 1. 
          Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. 
          If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit.
      language: 
        type: optional<string>
        docs: |
            The language of the input audio. 
            Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy and latency.
  CreateTranscriptionResponse: 
    docs: Note that this does not currently support the non-default response format types.
    properties:
      text: string
    