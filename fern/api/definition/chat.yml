# yaml-language-server: $schema=https://raw.githubusercontent.com/fern-api/fern/main/fern.schema.json

service:
  auth: true
  base-path: /chat
  endpoints:
    create:
      path: /completions
      method: POST
      docs: Creates a completion for the chat message
      request:
        body: CreateChatCompletionRequest
      response: CreateChatCompletionResponse

types:
  CreateChatCompletionRequest:
    properties:
      model:
        type: string
        docs: ID of the model to use.
      messages:
        type: list<ChatCompletionRequestMessage>
        docs: The messages to generate chat completions for, in the [chat format](https://platform.openai.com/docs/guides/chat/introduction).
      temperature:
        type: optional<double>
        docs: Minimum `0` and maximum `2` #TODO completions_temperature_description
      top_p:
        type: optional<double>
        docs: Minimum `0` and maximum `1` #TODO completions_top_p_description
      n:
        type: optional<integer>
        docs:
          How many chat completion choices to generate for each input message.
          Minimum `1` and maximum `128`
      stream:
        type: optional<boolean>
        docs: >
          If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
          as they become available, with the stream terminated by a `data: [DONE]` message.
      stop:
        type: optional<list<string>>
        docs: Up to 4 sequences where the API will stop generating further tokens.
      max_tokens:
        type: optional<integer>
        docs: The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens).
      presence_penalty:
        type: optional<double>
        docs: Minimum `-2` and maximum `2` #TODO completions_presence_penalty_description
      frequency_penalty:
        type: optional<double>
        docs: Minimum `-2` and maximum `2` #TODO completions_frequency_penalty_description
      logit_bias:
        type: optional<map<string,integer>>
        docs: |
          Modify the likelihood of specified tokens appearing in the completion.
          Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. 
          Mathematically, the bias is added to the logits generated by the model prior to sampling. 
          The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
      user:
        docs: >
          A unique identifier representing your end-user, which can help OpenAI
          to monitor and detect abuse. [Learn
          more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).
        type: optional<string>

  ChatModel:
    docs: These are the models currently supported.
    enum:
      - value: gpt-3.5-turbo
        name: gpt_3_5_turbo
      - value: gpt-3.5-turbo-0301
        name: gpt_3_5_turbo_0301
  ChatCompletionRequestMessage:
    properties:
      role:
        type: AuthorRole
        docs: The role of the author of this message.
      content:
        type: string
        docs: The contents of the message
      name:
        type: optional<string>
        docs: The name of the user in a multi-user chat

  AuthorRole:
    enum:
      - system
      - user
      - assistant

  CreateChatCompletionResponse:
    properties:
      id: string
      object: string
      created: integer
      model: string
      choices: list<ChatItems>
      usage: optional<ChatUsage>

  ChatItems:
    properties:
      index: integer
      message: ChatCompletionResponseMessage
      finish_reason: string

  ChatCompletionResponseMessage:
    properties:
      role:
        type: AuthorRole
        docs: The role of the author of this message.
      content:
        type: string
        docs: The contents of the message

  ChatUsage:
    properties:
      prompt_tokens: integer
      completion_tokens: integer
      total_tokens: integer
