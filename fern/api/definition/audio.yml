# yaml-language-server: $schema=https://raw.githubusercontent.com/fern-api/fern/main/fern.schema.json

imports:
  commons: commons.yml

service:
  auth: true
  base-path: /audio
  endpoints:
    transcribe:
      path: /transcriptions
      method: POST
      docs: Transcribes audio into the input language.
      request:
        name: CreateTranscriptionRequest
        body:
          properties:
            file:
              type: file
              docs: |
                The audio file to transcribe, in one of these formats: mp3, mp4, mpeg, mpga, m4a, wav, or webm.
            model:
              type: string
              docs: |
                ID of the model to use. Only `whisper-1` is currently available.
            prompt:
              type: optional<string>
              docs: |
                An optional text to guide the model's style or continue a previous audio segment. 
                The [prompt](https://platform.openai.com/docs/guides/speech-to-text/prompting) should match the audio language.
            response_format:
              type: optional<commons.FileFormat>
              docs: |
                The format of the transcript output.
            temperature:
              type: optional<double>
              docs: |
                The sampling temperature, between 0 and 1. 
                Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. 
                If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit.
            language:
              type: optional<string>
              docs: |
                The language of the input audio. 
                Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy and latency.
      response: CreateTranscriptionResponse

    translate:
      path: /translations
      method: POST
      docs: Translates audio into into English.
      request:
        name: CreateTranslationRequest
        body:
          properties:
            file:
              type: file
              docs: | 
                The audio file to translate, in one of these formats: mp3, mp4, mpeg, mpga, m4a, wav, or webm.
            model:
              type: string
              docs: |
                ID of the model to use. Only `whisper-1` is currently available.
            prompt:
              type: optional<string>
              docs: |
                An optional text to guide the model's style or continue a previous audio segment. 
                The [prompt](https://platform.openai.com/docs/guides/speech-to-text/prompting) should match the audio language.
            response_format:
              type: optional<commons.FileFormat>
              docs: |
                The format of the transcript output.
            temperature:
              type: optional<double>
              docs: |
                The sampling temperature, between 0 and 1. 
                Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. 
                If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit.
            response: CreateTranslationResponse

types:
  CreateTranscriptionResponse:
    docs: Note that this does not currently support the non-default response format types.
    properties:
      text: string

  CreateTranslationResponse:
    docs: Note that this does not currently support the non-default response format types.
    properties:
      text: string
